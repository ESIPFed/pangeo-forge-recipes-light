{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIB2 Reference Recipe for HRRR (High-Resolution Rapid Refresh)\n",
    "\n",
    "In this notebook, we will demonstrate how to create a reference recipe using GRIB2 files. As with all reference recipes, the original data is not duplicated, instead a reference/index of the dataset is built so the dataset can be read as if it were a Zarr store.\n",
    "\n",
    "The input files for this recipe are GRIB2 files provided by NOAA and stored in Amazon S3 ([HRRR AWS Open Data Page](https://registry.opendata.aws/noaa-hrrr-pds/)).\n",
    "\n",
    "This Pangeo-Forge tutorial is an adaptation of the [Kerchunk GRIB2 Project Pythia Cookbook](https://projectpythia.org/kerchunk-cookbook/notebooks/case_studies/HRRR.html). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the FilePattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import xarray as xr\n",
    "from pangeo_forge_recipes.patterns import pattern_from_file_sequence\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", anon=True, skip_instance_cache=True)\n",
    "\n",
    "# retrieve list of available days in archive\n",
    "days_available = fs.glob(\"s3://noaa-hrrr-bdp-pds/hrrr.*\")\n",
    "\n",
    "# Read HRRR GRIB2 files from latest day, the select the first 2\n",
    "files = fs.glob(f\"s3://{days_available[-1]}/conus/*wrfsfcf01.grib2\")[0:2]\n",
    "\n",
    "# Create a filepattern object from input file paths\n",
    "pattern = pattern_from_file_sequence(['s3://' + path for path in files], 'time', file_type='grib')\n",
    "pattern\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Examine an input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import s3fs\n",
    "# import xarray as xr \n",
    "# url = f'simplecache::s3://{files[0]}'\n",
    "# file = fsspec.open_local(url, s3={'anon': True}, filecache={'cache_storage':'/tmp/files'})\n",
    "\n",
    "# ds = xr.open_dataset(file, engine=\"cfgrib\", backend_kwargs={'filter_by_keys': grib_filters})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the Recipe\n",
    "\n",
    "Now that we have created our `FilePattern`,  we can build our `beam` pipeline. A beam pipeline is a chained together list of (Apache Beam transformations)[https://beam.apache.org/documentation/programming-guide/#transforms].\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify where our target data should be written\n",
    "Here, we are creating a temporary directory to store the written reference files. If we wanted these reference files to persist locally, we would want to specify another file path. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "td = TemporaryDirectory()\n",
    "target_root = td.name\n",
    "store_name = \"output.json\"\n",
    "target_store = os.path.join(target_root, store_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify additional args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grib_filters ={\"typeOfLevel\": \"heightAboveGround\", \"level\": [2, 10]}\n",
    "storage_options = {\"anon\": True}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a Pipeline\n",
    "Next, we will construct a beam pipeline. This should look similar to the other standard Zarr examples, but will involve a few different transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from pangeo_forge_recipes.transforms import OpenURLWithFSSpec, OpenWithKerchunk, DropKeys, CombineReferences, WriteCombinedReference\n",
    "\n",
    "store_name = \"GRIB2_reference\"\n",
    "output_json_fname = \"reference.json\"\n",
    "remote_protocol = \"s3\"\n",
    "transforms = (\n",
    "        # Create a beam PCollection from our input file pattern\n",
    "        beam.Create(pattern.items())\n",
    "        # Pass out file inputs to fsspec\n",
    "        | OpenURLWithFSSpec(open_kwargs={'anon':True})\n",
    "        # Pass our fsspec-opened files to Kerchunk to create references for each file\n",
    "        | OpenWithKerchunk(file_type=pattern.file_type, remote_protocol=remote_protocol)\n",
    "        # Minor transform (REQUIRED) to drop keys from the PCollection prior to combining\n",
    "        | DropKeys()\n",
    "        # Use Kerchunk's `MultiZarrToZarr` functionality to combine the reference files into a single reference file\n",
    "        # Note: Setting the correct contact_dims and identical_dims is important. \n",
    "        | CombineReferences(concat_dims=[\"valid_time\"],\n",
    "                            identical_dims=[\"latitude\", \"longitude\", \"heightAboveGround\", \"step\"],\n",
    "\n",
    "                            mzz_kwargs = {\"remote_protocol\": remote_protocol} )\n",
    "        # Write the combined Kerchunk reference to file storage\n",
    "        | WriteCombinedReference(\n",
    "            target_root=target_root,\n",
    "            store_name=store_name,\n",
    "            output_json_fname=output_json_fname,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    p | transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the Result\n",
    "\n",
    "Here we are creating an fsspec mapper of the reference file and then passing it to Xarray's `open_dataset` to be read as if it were a Zarr store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset as zarr object using fsspec reference file system and Xarray\n",
    "fpath = target_root + \"/\"+store_name +\"/\"+ output_json_fname\n",
    "fs = fsspec.filesystem(\n",
    "    \"reference\", fo=fpath\n",
    ")\n",
    "ds = xr.open_dataset(\n",
    "    fs.get_mapper(\"\"), engine=\"zarr\", backend_kwargs=dict(consolidated=False), chunks={\"valid_time\": 1}\n",
    ")\n",
    "ds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"t2m\"][-1].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo-forge-recipes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
