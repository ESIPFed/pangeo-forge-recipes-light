{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49caf2b2",
   "metadata": {},
   "source": [
    "# HDF Reference Recipe for CMIP6\n",
    "\n",
    "This example illustrates how to create a Reference Recipe using CMIP6 data.\n",
    "This recipe does not actually copy the original source data.\n",
    "Instead, it generates metadata files which reference and index the original data, allowing it to be accessed more efficiently. It does this by using the Python library, [Kerchunk](https://fsspec.github.io/kerchunk/) under the hood. Pangeo-Forge is acting as a runner for Kerchunk to generate reference files. \n",
    "For more background, see [this blog post](https://medium.com/pangeo/fake-it-until-you-make-it-reading-goes-netcdf4-data-on-aws-s3-as-zarr-for-rapid-data-access-61e33f8fe685).\n",
    "\n",
    "As the input for this recipe, we will use some CMIP6 NetCDF4 files provided by ESGF and stored in Amazon S3 ([CMIP6 AWS Open Data Page](https://registry.opendata.aws/cmip6/)).\n",
    "Many CMIP6 simulations spread their outputs over many HDF5/ NetCDF4 files, in order to limit the individual file size.\n",
    "This can be inconvenient for analysis.\n",
    "In this recipe, we will see how to virtually concatenate many HDF5 files into one big virtual Zarr dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f9855",
   "metadata": {},
   "source": [
    "## Define the FilePattern\n",
    "\n",
    "Let's pick a random dataset: ocean model output from the GFDL ocean model from the [OMIP](https://www.wcrp-climate.org/modelling-wgcm-mip-catalogue/cmip6-endorsed-mips-article/1063-modelling-cmip6-omip) experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965272cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=True)\n",
    "base_path = 's3://esgf-world/CMIP6/OMIP/NOAA-GFDL/GFDL-CM4/omip1/r1i1p1f1/Omon/thetao/gr/v20180701/'\n",
    "all_paths = fs.ls(base_path)\n",
    "all_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba062f69-5c31-44e9-a252-6b55e292b4e5",
   "metadata": {},
   "source": [
    "We see there are 15 individual NetCDF files. Let's time how long it takes to open and display one of them using Xarray.\n",
    "\n",
    "```{note}\n",
    "The argument `decode_coords='all'` helps Xarray promote all of the `_bnds` variables to coordinates (rather than data variables).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce8337-8984-43b5-bc01-33c531bb21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_orig = xr.open_dataset(fs.open(all_paths[0]), engine='h5netcdf', chunks={}, decode_coords='all')\n",
    "ds_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de47e7f-5196-4b98-b05d-f281cb2eb056",
   "metadata": {},
   "source": [
    "It took ~30 seconds to open this one dataset. So it would take 7-8 minutes for us to open every file. This would be annoyingly slow.\n",
    "\n",
    "As a first step in our recipe, we create a `File Pattern <../../recipe_user_guide/file_patterns>` to represent the input files.\n",
    "In this case, since we already have a list of inputs, we just use the `pattern_from_file_sequence` convenience function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_forge_recipes.patterns import pattern_from_file_sequence\n",
    "pattern = pattern_from_file_sequence(['s3://' + path for path in all_paths], 'time')\n",
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fa5d0a3-fdee-4072-a621-b905427cd616",
   "metadata": {},
   "source": [
    "## Write the Recipe\n",
    "\n",
    "Once we have our `FilePattern`, describing our input file paths, we can construct out `beam` pipeline. A beam pipeline is a chained together list of (Apache Beam transformations)[https://beam.apache.org/documentation/programming-guide/#transforms].\n",
    "\n",
    "\n",
    "<!-- The only custom options we need are to specify that we'll be accessing the source files anonymously and to use `decode_coords='all'` when opening them. -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a443948",
   "metadata": {},
   "source": [
    "### Specify where our target data should be written\n",
    "Here, we are creating a temporary directory to store the written reference files. If we wanted these reference files to persist locally, we would want to specify another file path. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "td = TemporaryDirectory()\n",
    "target_root = td.name\n",
    "store_name = \"output.json\"\n",
    "target_store = os.path.join(target_root, store_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b9d27c7",
   "metadata": {},
   "source": [
    "## Construct a Pipeline\n",
    "Next, we will construct a beam pipeline. This should look similar to the other standard Zarr examples, but will involve a few different transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebe82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from pangeo_forge_recipes.transforms import OpenURLWithFSSpec, OpenWithKerchunk, DropKeys, CombineReferences, WriteCombinedReference\n",
    "\n",
    "store_name = \"cmip6_reference\"\n",
    "output_json_fname = \"reference.json\"\n",
    "transforms = (\n",
    "        # Create a beam PCollection from our input file pattern\n",
    "        beam.Create(pattern.items())\n",
    "        # Pass out file inputs to fsspec\n",
    "        | OpenURLWithFSSpec(open_kwargs={'anon':True})\n",
    "        # Pass our fsspec-opened files to Kerchunk to create references for each file\n",
    "        | OpenWithKerchunk(file_type=pattern.file_type)\n",
    "        # Minor transform (REQUIRED) to drop keys from the PCollection prior to combining\n",
    "        | DropKeys()\n",
    "        # Use Kerchunk's `MultiZarrToZarr` functionality to combine the reference files into a single reference file\n",
    "        # Note: Setting the correct contact_dims and identical_dims is important. \n",
    "        | CombineReferences(concat_dims=[\"time\"], \n",
    "                            identical_dims=[\"lat\", \"lat_bnds\", \"lon\", \"lon_bnds\", \"lev_bnds\", \"lev\"],\n",
    "                            mzz_kwargs = {\"remote_protocol\": \"s3\"} )\n",
    "        # Write the combined Kerchunk reference to file storage\n",
    "        | WriteCombinedReference(\n",
    "            target_root=target_root,\n",
    "            store_name=store_name,\n",
    "            output_json_fname=output_json_fname,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a1e6228",
   "metadata": {},
   "source": [
    "## Execute the Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a08c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    p | transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "300f5b49-4b3c-4dc5-8519-63485456af94",
   "metadata": {},
   "source": [
    "## Examine the Result\n",
    "\n",
    "Here we are creating an fsspec mapper of the reference file and then passing it to Xarray's `open_dataset` to be read as if it were a Zarr store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d262ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec \n",
    "import xarray as xr\n",
    "full_path = os.path.join(target_root, store_name, output_json_fname)\n",
    "mapper = fsspec.get_mapper(\"reference://\", fo=full_path, remote_protocol=\"s3\",)\n",
    "ds = xr.open_dataset(mapper, engine=\"zarr\", decode_coords='all', backend_kwargs={\"consolidated\": False})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "206619cc",
   "metadata": {},
   "source": [
    "## Make a Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95c115-3cff-4454-83fd-8d9b89a77560",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ann = ds.resample(time='A').mean()\n",
    "sst_diff = ds_ann.thetao.isel(time=-1, lev=0) - ds_ann.thetao.isel(time=0, lev=0)\n",
    "sst_diff.plot()"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "allow_errors": false,
   "timeout": 3000
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
